---
layout: default
---

<head>
    <link rel="stylesheet" href="../../css/index.css">
</head>

<body>
<!-- Side navigation -->
<div class="sidenav">
    <a href="../../index.html" class="nav1">Introduction</a>
    <a href="../gettingstarted.html" class="nav1">Getting started</a>
    <a href="../modulelist.html" class="nav1">Modules</a>
    <a href="../macrolist.html" class="nav1">Macros</a>
    <!-- <a href="../extendingmia.html" class="nav1">Extending MIA</a> -->
</div>

<!-- Page content -->
<div class="main">
    <!--The following is replaced automatically by the generated module list-->
    <a href="../modulelist.html">Back to module list</a>
<h1>Affine (SIFT)</h1>
<h2>Description</h2>
Apply slice-by-slice (2D) affine-based image registration to a multi-dimensional stack.  Images can be aligned relative to the first frame in the stack, the previous frame or a separate image in the workspace.  The registration transform can also be calculated from a separate stack to the one that it will be applied to.  Registration can be performed along either the time or Z axes.  The non-registered axis (e.g. time axis when registering in Z) can be "linked" (all frames given the same registration) or "independent" (each stack registered separately).<br><br>This module uses the <a href="https://imagej.net/Feature_Extraction">Feature Extraction</a> plugin and associated MPICBG tools to detect SIFT ("Scale Invariant Feature Transform") features from the input images and calculate and apply the necessary 2D affine transforms.<br><br>Note: The SIFT-algorithm is protected by U.S. Patent 6,711,293: Method and apparatus for identifying scale invariant features in an image and use of same for locating an object in an image by the University of British Columbia. That is, for commercial applications the permission of the author is required. Anything else is published under the terms of the GPL, so feel free to use it for academic or personal purposes.<br><br>References:<ul><li>Lowe, David G. "Object recognition from local scale-invariant features". <i>Proceedings of the International Conference on Computer Vision</i> <b>2</b> (1999) 1150–1157.</li><li>Lowe, David G. "Distinctive Image Features from Scale-Invariant Keypoints". <i>International Journal of Computer Vision</i> <b>60</b> (2004) 91–110.</li></ul>
<h2>Parameters</h2>
<ul><li><b>Input image</b> (default = "") Image from workspace to apply registration to.</li><br><li><b>Apply to input image</b> (default = "true") When selected, the post-operation image will overwrite the input image in the workspace.  Otherwise, the image will be saved to the workspace with the name specified by the "Output image" parameter.</li><br><li><b>Output image</b> (default = "") If "Apply to input image" is not selected, the post-operation image will be saved to the workspace with this name.</li><br><li><b>Registration axis</b> (default = "Time") Controls which stack axis the registration will be applied in.  For example, when "Time" is selected, all images along the time axis will be aligned.  Choices are: Time, Z.</li><br><li><b>Other axis mode</b> (default = "Independent") For stacks with non-registration axis lengths longer than 1 (e.g. the "Z" axis when registering in time) the behaviour of this other axis is controlled by this parameter:<br><ul><li>"Independent" Each non-registration axis is registered independently.  For example, applying separate Z-registrations for each timepoint of a 4D stack.</li><li>"Linked" All elements of the non-registration axis are registered with a single transform.  For example, applying the same registration at a timepoint to all slices of a 4D stack.</li></ul></li><br><li><b>Fill mode</b> (default = "Black") Controls what intensity any border pixels will have.  "Borders" in this case correspond to strips/wedges at the image edge corresponding to regions outside the initial image (e.g. the right-side of an output image when the input was translated to the left).   Choices are: Black, White.</li><br><li><b>Enable multithreading</b> (default = "true") When selected, certain parts of the registration process will be run on multiple threads of the CPU.  This can provide a speed improvement when working on a computer with a multi-core CPU.</li><br><li><b>Reference mode</b> (default = "First frame") Controls what reference image each image will be compared to:<br><ul><li>"First frame" All images will be compared to the first frame (or slice when in Z-axis mode).  For image sequences which continuously evolve over time (e.g. cells dividing) this can lead to reduced likelihood of successfully calculating the transform over time.</li><li>"Previous N frames" Each image will be compared to the N frames (or slice when in Z-axis mode) immediately before it (number of frames specified by "Number of previous frames").  These reference frames are consolidated into a single reference image using a projection based on the statistic specified by "Previous frames statistic".  This mode copes better with image sequences which continuously evolve over time, but can also lead to compounding errors over time (errors in registration get propagated to all remaining slices).</li><li>"Specific image" All images will be compared to a separate 2D image from the workspace.  The image to compare to is selected using the "Reference image" parameter.</li></ul></li><br><li><b>Number of previous frames</b> (default = "1") Number of previous frames (or slices) to use as reference image when "Reference mode" is set to "Previous N frames".  If there are insufficient previous frames (e.g. towards the beginning of the stack) the maximum available frames will be used.  Irrespective of the number of frames used, the images will be projected into a single reference image using the statistic specified by "Previous frames statistic".</li><br><li><b>Previous frames statistic</b> (default = "Maximum") Statistic to use when combining multiple previous frames as a reference ("Reference mode" set to "Previous N frames").</li><br><li><b>Reference image</b> (default = "") If "Reference mode" is set to "Specific image" mode, all input images will be registered relative to this image.  This image must only have a single channel, slice and timepoint.</li><br><li><b>Calculation source</b> (default = "Internal") Controls whether the input image will be used to calculate the registration transform or whether it will be determined from a separate image:<br><ul><li>"External" The transform is calculated from a separate image from the workspace (specified using "External source").  This could be an image with enhanced contrast (to enable better feature extraction), but where the enhancements are not desired in the output registered image.  When "Other axis mode" is set to "Linked", the external image must be the same length along the registration axis and have single-valued length along the non-registration axis.  However, when set to "Independent", the external image must have the same axis lengths for both the registration and non-registration axes.</li><li>"Internal" The transform is calculated from the input image.</li></ul></li><br><li><b>External source</b> (default = "") If "Calculation source" is set to "External", registration transforms will be calculated using this image from the workspace.  This image will be unaffected by the process.</li><br><li><b>Calculation channel</b> (default = "1") If calculating the registration transform from a multi-channel image stack, the transform will be determined from this channel only.  Irrespectively, for multi-channel image stacks, the calculated transform will be applied equally to all channels.</li><br><li><b>Show detected points</b> (default = "false") When enabled, the points used for calculation of the registration will be added as an overlay to the input image and displayed.</li><br><li><b>Transformation mode</b> (default = "Rigid (trans., rot.)") Controls the type of registration being applied:<br><ul><li>"Affine (trans., rot., scale, shear)" Applies the full affine transformation, whereby the input image can undergo translation, rotation, reflection, scaling and shear.</li><li>"Rigid (trans., rot.)" Applies only translation and rotation to the input image.  As such, all features should remain the same size.</li><li>"Similarity (trans., rot., iso-scale)" Applies translation, rotating and linear scaling to the input image.</li><li>"Translation" Applies only translation (motion within the 2D plane) to the input image.</li></ul></li><br><li><b>Test flip (mirror image)</b> (default = "false") When selected, alignment will be tested for both the "normal" and "flipped" (mirror) states of the image.  The state yielding the lower cost to alignment will be retained.</li><br><li><b>Initial Gaussian blur (px)</b> (default = "1.6") "Accurate localization of keypoints requires initial smoothing of the image. If your images are blurred already, you might lower the initial blur ?0 slightly to get more but eventually less stable keypoints. Increasing ?0 increases the computational cost for Gaussian blur, setting it to ?0=3.2px is equivalent to keep ?0=1.6px and use half maximum image size. Tip: Keep the default value ?0=1.6px as suggested by Lowe (2004).".  Description taken from <a href="https://imagej.net/Feature_Extraction">https://imagej.net/Feature_Extraction</a></li><br><li><b>Steps per scale</b> (default = "3") "Keypoint candidates are extracted at all scales between maximum image size and minimum image size. This Scale Space is represented in octaves each covering a fixed number of discrete scale steps from ?0 to 2?0. More steps result in more but eventually less stable keypoint candidates. Tip: Keep 3 as suggested by Lowe (2004) and do not use more than 10.".  Description taken from <a href="https://imagej.net/Feature_Extraction">https://imagej.net/Feature_Extraction</a></li><br><li><b>Minimum image size (px)</b> (default = "64") "The Scale Space stops if the size of the octave would be smaller than minimum image size. Tip: Increase the minimum size to discard large features (i.e. those extracted from looking at an image from far, such as the overall shape).".  Description taken from <a href="https://imagej.net/Feature_Extraction">https://imagej.net/Feature_Extraction</a></li><br><li><b>Maximum image size (px)</b> (default = "1024") "The Scale Space starts with the first octave equal or smaller than the maximum image size. Tip: By reducing the size, fine scaled features will be discarded. Increasing the size beyond that of the actual images has no effect.".  Description taken from <a href="https://imagej.net/Feature_Extraction">https://imagej.net/Feature_Extraction</a></li><br><li><b>Feature descriptor size</b> (default = "4") "The SIFT-descriptor consists of n×n gradient histograms, each from a 4×4px block. n is this value. Lowe (2004) uses n=4. We found larger descriptors with n=8 perform better for Transmission Electron Micrographs from serial sections.".  Description taken from <a href="https://imagej.net/Feature_Extraction">https://imagej.net/Feature_Extraction</a></li><br><li><b>Feature descriptor orientation bins</b> (default = "8") "For SIFT-descriptors, this is the number of orientation bins b per 4×4px block as described above. Tip: Keep the default value b=8 as suggested by Lowe (2004).".  Description taken from <a href="https://imagej.net/Feature_Extraction">https://imagej.net/Feature_Extraction</a></li><br><li><b>Closest/next closest ratio</b> (default = "0.92") "Correspondence candidates from local descriptor matching are accepted only if the Euclidean distance to the nearest neighbour is significantly smaller than that to the next nearest neighbour. Lowe (2004) suggests a ratio of r=0.8 which requires some increase when matching things that appear significantly distorted.".  Description taken from <a href="https://imagej.net/Feature_Extraction">https://imagej.net/Feature_Extraction</a></li><br><li><b>Maximal alignment error (px)</b> (default = "25.0") "Matching local descriptors gives many false positives, but true positives are consistent with respect to a common transformation while false positives are not. This consistent set and the underlying transformation are identified using RANSAC. This value is the maximal allowed transfer error of a match to be counted as a good one. Tip: Set this to about 10% of the image size.".  Description taken from <a href="https://imagej.net/Feature_Extraction">https://imagej.net/Feature_Extraction</a></li><br><li><b>Inlier ratio</b> (default = "0.05") "The ratio of the number of true matches to the number of all matches including both true and false used by RANSAC. 0.05 means that minimally 5% of all matches are expected to be good while 0.9 requires that 90% of the matches were good. Only transformations with this minimal ratio of true consent matches are accepted. Tip: Do not go below 0.05 (and only if 5% is more than about 7 matches) except with a very small maximal alignment error to avoid wrong solutions.".  Description taken from <a href="https://imagej.net/Feature_Extraction">https://imagej.net/Feature_Extraction</a></li><br></ul>

</div>
</body>